# 语音合成

### 1 text->wav分两步：

#### 1.1 从文本中攫取足量信息（预处理）

文本结构与语种判断->文本标准化->文本转音素->句读韵律预测

- 音素起止时间，频率变化...
- 音节级别（单词的第几个音节）
- 单词级别（词性／在句子中的位置）
- 
  拼写相同但读音不同的词的区分、缩写的处理、停顿位置的确定，等等。

#### 1.2 生成波形

两种思路

准备材料：语音库corpus,包括用工具得到的音素和音频时间上的切分，即每个音素的语言学标注+在音频中对应的起止时间+

##### 思路1: 做个语音库，从库里面找找有没有合适的 speech unit，拼起来

- 同样的语言学标注系统跑一遍输入文本，得到了一串语言学标注。

- 从库里面找，有没有不仅在语言学特征上，还在声学特征上也是类似的音素 waveform。

- 找到了，拼起来，找不到，看看退而求其次的音素

- 缺点：如果库里的音素切分出错、语言学标注出错，那最后会找错。优点是，听起来的确很自然，毕竟是真人的声音。

  

##### 思路2:做个语音库，用统计模型学习到每个音到底怎么发的，再根据学出来的特征，复原出来。

- 基于参数的语音合成系统。文本抽象成语音学特征，再用统计学模型学习出来语音学特征和其声学特征的对应关系后，再从预测出来的声学特征还原成 waveform 的过程。
- 有若干统计模型可以解决，目前主流是用神经网络用来预测。然后用声码器 (vocoder) 生成波形，实现特征到 waveform 这最后一步。
- 缺点：听起来不自然，因为最后输出的是用声码器合成的声音，有损失。优点是，对于语音库里的标注错误不敏感，因为预测时候是学的是一个统计模型

##### 思路3:取两种思路的优点，混合的语音合成解决方案。用基于参数的语音合成系统预测声学上最匹配的音素后，再从库里把它找出来。

##### 思路4:端到端

用神经网络直接学习文本端到声学特征这一端的对应关系，这就直接省去了第一步，不再需要语言学标注系统标注文本了。这就是 Google 的 [Tacotron](https://link.zhihu.com/?target=https%3A//google.github.io/tacotron/)。不过最后还是要需要声码器。再或者，用神经网络直接学习语言学标注端到帧级别的 waveform 端的对应关系，这就直接省去了最后一步，不再需要声码器了。这就是 DeepMind 的 [WaveNet](https://link.zhihu.com/?target=https%3A//deepmind.com/blog/wavenet-generative-model-raw-audio/)。不过第一步还是需要语言学标注系统。



### 2 衡量指标

- 效果指标
  - MOS
  - ABX
- 性能指标
  - 实时率
  - 首包响应时间
  - 并发数
  - 合成一百词所需要时间



















